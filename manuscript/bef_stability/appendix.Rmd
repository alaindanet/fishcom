---
title: Appendix of Complex mecanisms linking biomass stability and total biomass with species richness and network structure in stream ecosystems 
author: Alain Danet, Maud Mouchet, Elisa Thebault and Colin Fontaine
date: \today 
output:
  pdf_document:
    fig_caption: true 
    keep_tex: true
fontsize: 12pt
header-includes:
   - \usepackage{booktabs}
   - \usepackage{makecell}
   - \usepackage{float}
   - \usepackage{setspace}
   - \doublespacing
   - \usepackage{lineno}
   - \linenumbers
geometry: margin=2cm
bibliography: references.bib
---

```{r, echo = FALSE}
mypath <- rprojroot::find_package_root_file
data_common <- mypath("data")
dest_dir <- mypath("data", "species")

knitr::opts_chunk$set(
  cache = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.dim = c(7, 7),
  fig.show = "hold",
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

library(tidyverse)
library(magrittr)
library(ggeffects)
library(cowplot)
library(kableExtra)
source(mypath("R", "misc.R"))
source(mypath("R", "plot_methods.R"))
source(mypath("R", "community_analysis.R"))
source(mypath("R", "community_methods.R"))
source(mypath("R", "statistical_analysis.R"))
source(mypath("R", "total_sem_effect.R"))
source(mypath("R", "press_methods.R"))
source(mypath("R", "geo_methods.R"))
source(mypath("R", "synchrony.R"))

theme_set(theme_alain())
```

```{r}
# Load manuscript results
load(file = mypath("manuscript", "bef_stability", "result", "workspace.rda"))
```

# Sampling protocols

## Fishing

### Protocol

```{r}

```

The fish community sampling took place in a variety of stream size, shape and
depth. This diversity constraints the type of protocol that can take place.
For the small streams (depth < 0.7m across the entire width), the complete protocol
is used. It consists in placing anodes every X meter across the width of the
river. **Add the precise procedure?**
The fishermen made from one to three row of fishing. We kept only the first row
for analysis.
For deeper and wider streams, the sampling was partial over the bank and the
channel and performed by boat and by foot (along the bank). We kept only
sampling done over the bank because channel sampling was not always done.
Two types of partial protocols takes place: partial over bank and partial by
points. The former is continuous while the latter is discrete. Furthermore, in
partial by point protocol the channel can be sampled as well. We kept only the
partial by point fish sampling which all the sampling point were done over the
bank to homogenize the sampling. Then, we merged partial over bank and partial
over point protocols. Some sampling stations had mix characteristics in which mix of
partial and complete protocols were applied depending of the depth of the river
at the time of sampling. Those stations were removed from the analysis.

### Batches definition and body size measurement 
During the electric fishing, and before that fish body size are measured, they
are regrouped by batches. Each batches contains one species but there can be
several batches by species. Body size assessment are different according to the
batches. There are four types of batches: N, I, S/L and G. The batches N and
I contains the biggest individuals, there are normally less numerous and then
the body size of the individuals of those batches is recorded. The batches S/L
contains fishes of intermediate sizes. Inside each S/L batches, the operator
tries to make batches as homogeneous as possible in body size. Then, the body
size of a subsample of X individuals are recorded. The G batches contain
individuals of low body size, with care that body size is as homogenous as
possible in the batches. Then body size of the shortest and the longest
individuals are recorded.

## Body size inference

```{r}
myload(fish_length, lot_id_opcod, dir = mypath("data-raw", "fishing_op_build"))

fish_length %<>%
  left_join(lot_id_opcod) %>%
  rename(length = fish) %>%
  select(opcod, species, length)
length_sem_analysis <- fish_length %>% 
  filter(opcod %in% op_analysis_bbb$opcod)
na_length <- sum(is.na(length_sem_analysis$length))
```

The instructions for body size inference were guided by the recommendations of
the OFB agency, which has defined the sampling protocols, supervised the data
collection and performed analysis for multiple purpose including stream
management.

```{r}
body_size_caption <- paste0(
  "Body size inference of the fishes. ",
  "(1) Sampled fishes were attributed to batch type according to their size and
  the number of individuals. A batch contains one species only. (2) In the batch
  type N and I, the body size of all the individuals was recorded. In the
  batches, S/L, the body size of a subsample of the individuals was recorded.
  Finally, only the body size of the littlest and of the tallest individual was
  recorded. ",
  "Then, (3) we inferred the body size distribution in the S/L and G batches by
  assuming that it followed a normal distribution. The mean and the standard
  deviation were estimated with the measured subsample."
) 
```

```{r p-body-size, fig.cap=body_size_caption}
knitr::include_graphics(mypath("manuscript", "bef_stability", "figs", "length_inference.pdf"))
```

With the information contained in the batch record, we inferred the body size of
the fishes in the community. No inference were needed for the N and I batches
because the body size of the individuals were recorded. The body size ($l$) of the
fishes in the S/L and G were inferred under the assumption that its distribution
followed a normal law ($l ~ N(\mu , \sigma)$).
For a S/L batch, the mean $\mu$ and the standard deviation $\sigma$ were simply
the average and the standard deviation of the body size of the subsample
recorded. For a G batch, the average was approximated by 
$\hat{\mu} = (l_{min} + l_{max}) 2^{-1}$ and the standard deviation by
$\hat{\sigma} = (l_{min} + l_{max}) 4^{-1}$. Together with the estimations of
the mean and the standard deviation, the number of individuals in a batch and
assumption of a normal distribution, we simulated the body size distribution in
S/L and G batches. To avoid simulation of unrealistic body size, we used a
truncated normal distribution bounded at [0.05; .95] for the density of
probability of the S/L batches, and bounded at [$l_{min}$; $l_{max}$] for the G
batches. Body size distribution were generated using the R function
`truncdist::rtrunc`.

We removed batches that did not satisfied the following conditions. First, we
removed batches which the total number of individuals was not recorded. In lot
"G", we removed batches were  $l_{min} \geq l_{max}$ or if at least one of the
measurement was missing. Additionally, G batches were removed when number of
individuals fell below five, as G batches normally contained more individuals.
In the S/L lot, we removed batches were the number of individuals in the
recorded individuals fell below 10. In sum, those sanity check allowed to
exclude potentially problematic batches. Finally, in the dataset used in the
analysis, `r na_length` individuals had missing length over 
`r nrow(fish_length)` individuals.

## Community data 

```{r}
source(mypath("R", "cleaning_methods.R"))
removed_ind <- filter(length_sem_analysis, species %in% sp_to_remove())
removed_sp <- unique(removed_ind$species)
```

We removed rare fish species, species from lake, crayfish species, and migratory
species. In total, we removed `r nrow(removed_ind)` over 
`r nrow(length_sem_analysis)` individuals and `r length(removed_sp)`.

During fish sampling, some species are identified based on morphotype but they
belong to the same species based on fecundation criteria (CHECK with Eric).
We merged those morphotype species into one species. There was
`r length(unique(names(sp_to_replace())))` to merge which belonged to 
`r length(unique(sp_to_replace()))` species.

## Network inference

```{r}
myload(metaweb_analysis, dir = mypath("data"))
species_in_metaweb <- metaweb_analysis$species

length_sem_analysis %<>%
  filter(!species %in% sp_to_remove()) %>%
  mutate(
    species = str_replace_all(species, sp_to_replace())
  )

removed_ind_metaweb <- length_sem_analysis %>%
  filter(!species %in% species_in_metaweb)
removed_sp_metaweb <- unique(removed_ind_metaweb$species)
```

The metaweb building was based on trait matching, so it necessitates to have
information about their diet. `r length(removed_sp_metaweb)` species were
removed because information about their diet was missing. It represented  
`r nrow(removed_ind_metaweb)` individuals over `r nrow(length_sem_analysis)` 
individuals.

** I question myself if I should move the network inference figure here and
just let the temporal network in the main text. May be even add the temporal
biomass plot (species/trophic species) in the main text**

## Environmental variables

The procedure to obtain altitude, slope, width depth of the streams are fully
explained in main text. Because those measurements were collected during the
fishing protocol, they were easier to manage in the analysis. Then, we detailled
in the following sections how water temperature, flow and BOD were obtained. 

### Preparation

Water temperature, flow and BOD (Biogical Oxygen Demand) were obtained from the
[naiades database](http://www.naiades.eaufrance.fr/).
Water temperature, flow and BOD were respectively collected at a hourly, daily
and monthly frequency. Water temperature and flow are recorded by automated
sendors while BOD is collected manually because it necessitates analysis in a
laboratory. BOD data collection is coordinated by the Agence de l'eau which
realized the analysis itself or order it to a company.
It is crucial to make clear that those variables are collected at different
places than the fish samplings.

- Number of station
- Space repartition

```{r}
myload(fourier, dir = mypath("data-raw", "polluants"))
# Where is DBO?
```

For BOD, we removed measurements that were expressed in uncommon units.

The first goal was to harmonize the different variables and scale it to the
timescale of fish sampling, we took annual average of each variables. To reduce 
daily or seasonal effects, we first applied average data with the moving
window of one year. For temperature, the moving was performed on daily average
temperature. Then we took the annual average of each year and each variable.
 
Several characteristics of the streams make the interpolation more complicated
than along terrestrial surface. Interpolations in terrestrial areas are roughly
based on euclidian distance. Euclidian distance is more complicated to consider
in streams because it had the follow the path of the streams. Futhermore,
two streams can be close by but not connected and the value of their variables
could be not related or a only a few. For those reason, we used stream spatial
network methods to interpolate temperature, flow and BOD.

We reconstructed the stream network and the connection between sites with the
`openSTARS` R package. The reconstruction was done with a raster of digital
elevation model at a resolution of 250 meters [(IGN)](https://wxs.ign.fr/jvam1hsjm11u8voorw81v2xb/telechargement/prepackage/BDALTI-75M_PACK_FXX_2018-01-24%24BDALTIV2_2-0_75M_ASC_LAMB93-IGN69_FRANCE_2018-01-15/file/BDALTIV2_2-0_75M_ASC_LAMB93-IGN69_FRANCE_2018-01-15.7z)
and a theoretical hydrologic network [put a link](test) [@pella_reseau_2012].

The procedure was done by hydrographic basin because of convergence problem
when the interpolation was done at the France scale. We had to make a tradeoff
between the number of data (to keep enough high to perform the interpolation)
and the level of dataset splitting. We then merge some hydrographic basin to end
up with four region of interpolation: West, East, South and North. (XXX ADD 
basins merged) 

Then we modeled each variable as dependant only on the covariation between sites
and taking in account stream network. To model the autocovariance between sites,
we used the linear-sill and mariah functions for taildown and tailup
autocovariance, according to their performance of prediction. The interpolations
were done with the `SSN` R package 
[@isaak_applications_2014, @hoef_ssn_2014].

### Performance and post check

```{r eval = FALSE}
myload(cv_press_interp_mv_avg,
  dir = mypath("data-raw", "polluants"))
myload(cv_temp_press_interp_mv_avg,
  dir = mypath("data-raw", "naiades_temperatures"))
myload(cv_flow_press_interp_mv_avg,
  dir = mypath("data-raw", "flow"))
dbo_cv <- cv_press_interp_mv_avg %>%
  filter(parameter == "dbo5")

cv_dbo_temp_flow <- map2_dfr(
  list(dbo = dbo_cv, temp = cv_temp_press_interp_mv_avg, flow = cv_flow_press_interp_mv_avg),
  c("BOD", "Temperature", "Flow"),
  function(x, y) {
    x %>%
      group_by(basin, year) %>%
      summarise_if(is.numeric, list(avg = ~mean(., na.rm = TRUE))) %>%
      mutate(Variable = y) %>%
      select(Variable, basin, year, everything()) %>%
      ungroup
  })


cv_dbo_temp_flow %>%
  mutate_if(is.double, ~signif(., 2)) %>%
  kable(
    align = "c",
    format = "latex",
    booktabs = T,
    caption = paste0("Results of the cross-validation for the interpolation of
      BOD, temperature and flow. Average over 4 interpolations are shown.") 
      ) %>%
    collapse_rows(columns = c(1, 2), valign = "top") %>%
    kable_styling(latex_options =c("striped", "scale_down"))
```

```{r}
cv_bias_caption <- paste0(
  "Bias of the interpolation from Leave-One-Out Cross Validation (LOOCV). "
)
```

```{r cv-biais, fig.cap = cv_bias_caption}
cv_dbo_temp_flow %>%
  rename(Bias = bias_avg) %>%
  ggplot(aes(x = Bias)) +
  geom_histogram() +
  facet_wrap(~Variable, scales = "free_x")
```

The performance of the interpolation was evaluated with leave one out
cross-validation (already implemented in the SSN package). The value of the
interpolated variable of each station was predicted after removing it from the
dataset. ADD SUMMARIES ABOUT CV.

With the model, we then simply predicted variable values at the location of the
fish stations.

When predicted values were below the minimum value or above the maximum value
recorded in the dataset for flow, (temperature?) and BOD, we replaced the
predicted value by respectively the minimum and the maximum value found in the
dataset. We ran the interpolation four times and we kept the predicted value
which had the least standard deviation. 



# Analysis

## Time series classication

The CV of biomass over can cover multiple dynamics, including increases,
stable, decreases and a mix of them. Theoretically, however, the CV computed to
quantify stability is often represented with the assumption that the overall
dynamic of the biomass varies around an equilibrium. In order to approach this
condition, we kept only the stations that have met this criteria. To qualify the
biomass trajectories over time, we used the symbolic aggregated approximation
method (SAX) which is commonly used in time series analysis and implemented in
the `jmotif` R package. This method classifies time series rather than
quantifies trends and so is generic enough to encompass the diversity of time
series shapes. The classification necessitates three steps. First, the time
series are normalized (i.e. substracting the mean and divide by the standard
deviation). The second step is to reduce the dimensionality of the time series
by Piecewise Aggregation Approximation (PAA) method. Each time series is divived
in $N$ frames of equal size that allows to the reduce the dimensionality of the
time series. For each frame, we simply kept the mean value of the time series.
The last step is to qualify each frame into string. Because values of
normalized time series typically follows a normal distribution ($N(0,1)$), a
breakpoints can be computed by dividing the area under the normal distribution
in equal parts. The algoritm attributes a alphabetic letter to each part from
the lowest to the highest ($a$, $b$, $c$, ...). We set three breakpoints, so
$a$, $b$ and $c$. Finally, the trajectory of each time series is classified as
a combination of $a$, $b$, $c$ letters. $abc$ and $cba$ time series represent
respectively monoton increasing and decreasing trajectories. For data analysis,
we kept only the $bbb$ which represent time series that showed no temporal
tendancies (with respect to our methodology).


## PCA of environmental variables

```{r, fig.show = "hide"}
axis_pair <- list(c(1,2), c(1,3), c(2,3), c(1, 4), c(2, 4), c(3,4), c(4,5))
p_l_rot <- map(axis_pair, 
  function (axis)
    pca_rotated_plot <- plot_rotated_pca(
      pca_rotated = pca_rotated,
      axis = axis 
    )
)
```

```{r}
prop_var_expl_pca <- pca_rotated$rotated$Vaccounted["Proportion Var", ] %>%
  map_dbl(., ~round(.x * 100))

tot_var_expl_pca <- round(sum(pca_rotated$rotated$Vaccounted["Proportion Var", ]) * 100) 

pca_caption <- paste0(
  "Rotated axis of PCA on environmental variables. The five components
  explained,", tot_var_expl_pca, "% of the variance (",
  paste0(names(prop_var_expl_pca), ":", prop_var_expl_pca, "%"), ").",
  "(A) Axis 1 was positively related to the river size, axis 2 was related to
  average temperature and negatively related to the river slope and altitude.
  (B) Axis 3 was negatively related to the CV of flow and average enrichment.
  Axis 4 was related to the CV of river size. Finally, axis 5 was related to the
  CV of enrichment.")
```


```{r pca-rotated, fig.cap = pca_caption}
to_plot <- c(1, 6, 7)
plot_grid(plotlist = map(p_l_rot[to_plot], ~.x$rotated),
  ncol = length(to_plot), labels = LETTERS[1:3])
```

```{r}
print_prop_var_pca <- prop_var_expl_pca[order(names(prop_var_expl_pca))]
```


The five axis of the rotated PCA explained respectively
`r paste0(prop_var_expl_pca, "%", collapse = ", ")` of the variance.
Please see the main text for the interpretation of the axis. 


## Model summary and diagnostics 


####  SEM indirect and total effect

```{r}
caption_indir_stab_sem <- paste0(
  "Indirect and total effect of variables on biomass stability from the
  structural equation model. Indirect effects are computed by multiplying the
  path coefficients from the variable predictor to the response variable, here
  biomass stability. Indirect effects are considered as one step from the target
  variable. For exemple, the indirect effect of species richness on
  biomass stability via its effect on CVsp is $-0.61$. To compute indirect
  effect, only the significant direct effects were taken into account. ",
  "Network structure being the indirect effect that went through both avg trophic
  level and connectance and Species richness_Network structure being the
  indirect effect that went through (i) species richness and from (ii) species
  richness to network structure."

)

caption_indir_bm_sem <- paste0(
  "Indirect and total effect of variables on total biomass from the
  structural equation model. See the table (X) for details about computation and
  meaning." 

)
```


```{r indir-tab, fig.cap = "Indirect effect from SEM"}
format_table <- function (x = tmp_stab_indir_table) {
  x <- x[, c("variable", names(x)[!names(x) %in% "variable"])]

  rm_n <- str_remove_all(get_sem_var_name_replacement(), pattern = "\n")
  names(rm_n) <- names(get_sem_var_name_replacement()) 
  x$variable <- str_replace_all(x$variable, rm_n)

  colnames(x) %<>% str_remove_all(., pattern = "via_")

  col_replacement <- c(
    variable = "Variable",
    cv_sp = "CVsp",
    sync = "Synchrony",
    richness = "Species richness",
    com = "Network structure",
    richness_com = "Sp. richness and network str.",
    total = "Total effect"
  )
  colnames(x) %<>% str_replace_all(., col_replacement)

  x
}

kable(format_table(tmp_stab_indir_table),
  "latex",
  booktabs = T,
  caption = caption_indir_stab_sem 
  ) %>%
kable_styling(latex_options =c("striped", "scale_down"))

```

```{r}

kable(format_table(tmp_bm_indir_table),
  "latex",
  booktabs = T,
  caption = caption_indir_bm_sem
  ) %>%
kable_styling(latex_options =c("striped", "scale_down"))
```

### Model linear Relationship


```{r}
hist_var_caption <- paste0(
  "Distribution of the variables used in the Structural Equation Modeling."
)
```


```{r hist-sem-data, fig.cap = hist_var_caption}
sem_data %>%
  select(log_bm_std, log_rich_tot_std, log_sync, log_stab_std, log_cv_sp, log_RC1, log_RC2, log_RC3, RC4,
    RC5, t_lvl, ct, log_bm) %>%
  gather(variable, value) %>%
  mutate(variable = str_replace_all(variable, get_sem_var_name_replacement())) %>%
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~variable, scales = "free")
```

### Colinearity tables

```{r s-colin-sem, fig.cap = "Variance Inflation factor for each individual model of stability sem"}

mod_stab_list <-
  list(
    nlme::lme(log_rich_tot_std ~ log_RC1 + log_RC2 + log_RC3 + RC4 + RC5,  random = ~ 1 | basin, data = sem_data),
    nlme::lme(ct ~ log_RC1 + log_RC2 + log_RC3 + RC4 + RC5 + log_rich_tot_std, random = ~ 1 | basin, data = sem_data),
    nlme::lme(t_lvl ~ log_RC1 + log_RC2 + log_RC3 + RC4 + RC5 + log_rich_tot_std, random = ~ 1 | basin, data = sem_data),
    nlme::lme(log_sync ~ log_rich_tot_std + ct + t_lvl + log_RC1 + log_RC2 + log_RC3 + RC4 + RC5,
      random = ~ 1 | basin, data = sem_data),
    nlme::lme(log_cv_sp ~ log_rich_tot_std  + ct + t_lvl
      + log_RC1 + log_RC2 + log_RC3 + RC4 + RC5, random = ~ 1 | basin, data = sem_data),
    lm(log_stab_std ~ log_cv_sp + log_sync, data = sem_data),
    nlme::lme(log_bm_std ~ log_rich_tot_std  + ct + t_lvl
      + log_RC1 + log_RC2 + log_RC3 + RC4 + RC5, random = ~ 1 | basin, data = sem_data)
  )
names(mod_stab_list) <- c(
  "Species richness",
  "Connectance",
  "Avg trophic level",
  "Synchrony",
  "CVsp",
  "Biomass stability",
  "Total biomass"
) 

colin_stab_sem <- map(mod_stab_list, car::vif)

tmp <- map(colin_stab_sem, enframe)
  
# colinearity table
colin_stab_sem <- tibble(vif = tmp, mod = names(mod_stab_list)) %>%
  unnest(vif) %>%
  mutate(
    value = round(value, 1),
    name = str_replace_all(name, get_sem_var_name_replacement())
    ) %>%
  spread(name, value)

kable(colin_stab_sem,
  format = "latex",
  booktabs = T,
  caption = caption_indir_bm_sem
  ) %>%
kable_styling(latex_options = c("striped", "scale_down"))
```

```{r}
caption_mod_res <- paste0(
  "Residuals versus fitted values of the linear models used in the structural
  equation modelings."
) 
```

```{r mod-res, fig.cap = caption_mod_res}
library(nlme)
plot_residuals <- function (x, y = "Diagnostic") {
  plot.lme(x, main = y, xlab = "Fitted values", ylab = "Residuals")
}
p_resid_stab_sem <- map2(mod_stab_list, names(mod_stab_list) , plot_residuals)
plot_grid(plotlist = p_resid_stab_sem)
```


### Biomass and stability models

```{r}
format_coef_table <- function (sem = NULL) {

  params <- sem$coefficients
  colnames(params)[length(colnames(params))] <- "Stars"

  replace_wo_n <- str_replace_all(get_sem_var_name_replacement(), "\n", "")
  names(replace_wo_n) <- names(get_sem_var_name_replacement()) 

  params %<>%
    mutate_at(c("Response", "Predictor"),
      ~str_replace_all(., replace_wo_n))

  params
}

```

```{r}
est_stab_sem_caption <- paste0(
  "Estimate of the effects for the biomass stability structural equation
  modeling."

)

kable(format_coef_table(stab_sem_rich),
  format = "latex",
  booktabs = T,
  caption = est_stab_sem_caption
  ) %>%
kable_styling(latex_options = c("striped", "scale_down"))
```

```{r}

est_bm_sem_caption <- paste0(
  "Estimate of the effects for the total biomass structural equation
  modeling."
)

kable(format_coef_table(bm_sem_rich),
  format = "latex",
  booktabs = T,
  caption = est_bm_sem_caption
  ) %>%
kable_styling(latex_options = c("striped", "scale_down"))
```

```{r rsq-model}
rm_n <- str_remove_all(get_sem_var_name_replacement(), pattern = "\n")
names(rm_n) <- names(get_sem_var_name_replacement()) 
rsq_table <- map_dfr(mod_stab_list, piecewiseSEM::rsquared, method = "nagelkerke") %>%
  mutate_at(vars(Conditional, Marginal), ~round(., 2)) %>%
  mutate(Response = str_replace_all(Response, rm_n)) %>%
  dplyr::select(Response, Marginal, Conditional, R.squared)
rsq_table_caption <- paste0(
  "R squared of the linear models used in the structural equation modeling."
)
kable(format_coef_table(bm_sem_rich),
  format = "latex",
  booktabs = T,
  caption = est_bm_sem_caption
  ) %>%
kable_styling(latex_options = c("striped"))
```



```{r tab-coeff-bm, fig.cap = "Coefficient"}

# Replacement rules for term
rp_est <- c(
  "(Intercept)" =     "intercept",
  "sd\\_\\(intercept\\)\\.basin" = "sd intercept basin",
  "log_rich" =     "Median richness (log base 10)",
  "troph_group3" =     "high trophic group",
  "log_rich:troph_group3" =     "Median richness (log10) : high trophic group",
  "sig01.basin" =     "sd intercept basin",
  "sd_Observation.Residual" =     "residuals",
  "sigma" = "residuals"
)
# Replacement rules for group
rp_group <- c(
  "fixed" = "Fixed",
  "basin" = "Random"
)

```

```{r}
mask_drop_lm <- names(mod_stab_list) %in% "Biomass stability"
random_coeff_sem <- map(mod_stab_list[!mask_drop_lm], random.effects)

format_random <- function (x, y) {
  tibble(
    Variable = y, 
    `Hydrographic basin` = rownames(x),
    Intercept = x[[1]]
  )
}
formated_random_effects <- do.call(rbind, map2(random_coeff_sem, names(random_coeff_sem), format_random))

caption_random_effect <- paste0(
  "Estimated random effect of hydrographic basin on the intercept of the linear
  models performed in structural equation modeling."

)

kable(formated_random_effects,
  format = "latex",
  booktabs = T,
  caption = caption_random_effect
  ) %>%
kable_styling(latex_options = c("striped", "scale_down"))
```


```{r}
knitr::opts_chunk$set(eval = FALSE)
```


```{r}
coeff_troph <- map2_dfr(list(bm_troph_ci, stab_troph_ci), list(bm_troph_mod, stab_troph_mod),
  clean_summary_mod, term_rp = rp_est, group_rp = rp_group)

kable(coeff_troph) %>%
  pack_rows("Biomass", 1, 6) %>%
  pack_rows("Stab", 7, 12)
```

## Relationship at the absolute scale, i.e. not the logarythme scale 

```{r}
ggplot(sem_data, aes(x = richness_med, y = biomass_stab)) +
  geom_point()
test <- tibble(
  richness_med = seq(1,20, length.out = 100),
  biomass_stab =
    10^(fixef(stab_div_mod)[1])*richness_med^fixef(stab_div_mod)[2]
)  
```

```{r save-plot, eval = FALSE}

save_plot("~/Documents/thesis/talks/fishcom_fig/stab_div.pdf",
  p_stab_div,
  base_height = 3,
  bg = "transparent",
  ncol = 1)

save_plot("~/Documents/thesis/talks/fishcom_fig/stab_div_troph.pdf",
  p_stab_div_troph + theme(legend.position = "bottom"),
  base_height = 3,
  bg = "transparent",
  ncol = 1)

```

### Ecological modeling fun

```{r, eval = FALSE}
mod_tlvl <- nlme::lme(t_lvl ~ scale(RC1) + scale(RC2) + scale(RC3) + scale(RC4) + scale(RC5) + log_rich_tot, random = ~ 1 | basin, data = sem_data)
plot(mod_tlvl)
sem_data %>%
  ggplot(aes(x = log10(richness_tot), y = t_lvl)) +
  geom_point()+
  geom_smooth(method = "lm")
plot(x = log10(sem_data$RC1 + abs(min(sem_data$RC1))), y = sem_data$t_lvl)
sem_data %>%
  ggplot(aes(x = log10(RC1), y = t_lvl)) +
  geom_point()+
  geom_smooth(method = "lm")
plot(nlme::lme(t_lvl ~ I(log10(RC1 + abs(min(RC1)))) + log_rich_tot, random = ~ 1 | basin, data = sem_data))
nlme::lme(t_lvl ~ log_rich_tot, random = ~ 1 | basin, data = sem_data)
semd_test <- sem_data %>%
  mutate(RC1b = RC1 + abs(min(RC1)))
ricker.mixed.fit <- nls(
  t_lvl ~ (Vm*(RC1 + b) / (K + (RC1 + b)) + d),
 #fixed = Vm + K + b + d ~ 1,
 #random = Vm + K + b + d ~ 1|basin,
 start= c(Vm=1, K = 0.1, b = .5, d = 2.5),
 data = semd_test,
 control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) 
)
summary(ricker.mixed.fit)
x <- seq(min(sem_data$RC1), max(sem_data$RC1), length.out = 100)
sem_data$test_pred <- predict(ricker.mixed.fit)

semd_test %>%
  ggplot(aes(x = RC1b, y = t_lvl)) +
  geom_point() +
  geom_line(aes(y = test_pred))
  geom_smooth(method = "lm")
cor(semd_test$piel, semd_test$t_lvl, method = c("spearman"))
```

```{r, eval = FALSE}
library(glmmTMB)
# Gaussian
mod_log_rich <- glmmTMB(
  log_rich_tot ~ log_RC1 + log_RC2 + log_RC3
  + RC4 + RC5 + (1 | basin), family = gaussian, data = sem_data)
summary(mod_log_rich)
plot(y = resid(mod_log_rich), x = fitted(mod_log_rich))

# Poisson 
mod_log_rich_poi <- glmmTMB(
  richness_tot ~ log_RC1 + log_RC2 + log_RC3
  + RC4 + RC5 + (1 | basin), family = poisson, data = sem_data)
summary(mod_log_rich_poi)
plot(y = resid(mod_log_rich_poi), x = fitted(mod_log_rich_poi))

# Truncate 
mod_log_rich_nbinom <- glmmTMB(
  richness_tot ~ log_RC1 + log_RC2 + log_RC3
  + RC4 + RC5 + (1 | basin), family = nbinom1, data = sem_data)
summary(mod_log_rich_nbinom)
plot(y = resid(mod_log_rich_nbinom), x = fitted(mod_log_rich_nbinom))
```
